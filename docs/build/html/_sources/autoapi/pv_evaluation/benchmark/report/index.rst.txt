:py:mod:`pv_evaluation.benchmark.report`
========================================

.. py:module:: pv_evaluation.benchmark.report


Module Contents
---------------


Functions
~~~~~~~~~

.. autoapisummary::

   pv_evaluation.benchmark.report.inventor_benchmark_table
   pv_evaluation.benchmark.report.inventor_benchmark_plot
   pv_evaluation.benchmark.report.style_cluster_inspection
   pv_evaluation.benchmark.report.inspect_clusters_to_split
   pv_evaluation.benchmark.report.inspect_clusters_to_merge



Attributes
~~~~~~~~~~

.. autoapisummary::

   pv_evaluation.benchmark.report.DEFAULT_BENCHMARKS
   pv_evaluation.benchmark.report.DEFAULT_METRICS


.. py:data:: DEFAULT_BENCHMARKS
   

   

.. py:data:: DEFAULT_METRICS
   

   

.. py:function:: inventor_benchmark_table(disambiguations, metrics=DEFAULT_METRICS, benchmarks=DEFAULT_BENCHMARKS)

   Compute performance evaluation metrics on benchmark datasets.

   :param disambiguations: dictionary of disambiguation results (disambiguation results are pandas Series with "mention-id" index and cluster assignment values).
   :type disambiguations: dict
   :param metrics: Dictionary of metrics (from the metrics submodule) to compute. Defaults to `DEFAULT_METRICS`.
   :type metrics: dict, optional
   :param benchmarks: Benchmark datasets loading functions (from the benchmark submodule) to use. Defaults to `DEFAULT_BENCHMARK`.
   :type benchmarks: dict, optional


.. py:function:: inventor_benchmark_plot(disambiguations, metrics=DEFAULT_METRICS, benchmarks=DEFAULT_BENCHMARKS, **kwargs)

   Bar plot of performance evaluation metrics on benchmark datasets.

   :param disambiguations: dictionary of disambiguation results (disambiguation results are pandas Series with "mention-id" index and cluster assignment values).
   :type disambiguations: dict
   :param metrics: Dictionary of metrics (from the metrics submodule) to compute. Defaults to `DEFAULT_METRICS`.
   :type metrics: dict, optional
   :param benchmarks: Benchmark datasets loading functions (from the benchmark submodule) to use. Defaults to `DEFAULT_BENCHMARK`.
   :type benchmarks: dict, optional

   :returns: plotly graph object


.. py:function:: style_cluster_inspection(table, by='prediction')

   Style table to highlight groups with alternating colors.

   :param table: DataFrame to style.
   :type table: dataframe
   :param by: column to color by. Defaults to "prediction".
   :type by: str, optional


.. py:function:: inspect_clusters_to_split(disambiguation, benchmark, join_with=None)

   Get table of cluster assignment errors on the given benchmark.

   :param disambiguation: Disambiguation result Series.
   :type disambiguation: Series
   :param benchmark: reference disambiguation Series.
   :type benchmark: Series
   :param join_with: DataFrame to join based on "mention-id". Defaults to None.
   :type join_with: DataFrame, optional

   :returns: DataFrame containing erroneous cluster assignments according to the given benchmark.
   :rtype: DataFrame


.. py:function:: inspect_clusters_to_merge(disambiguation, benchmark, join_with=None)

   Get table to inspect missing cluster links given a benchmark dataset.

   :param disambiguation: Disambiguation result Series.
   :type disambiguation: Series
   :param benchmark: reference disambiguation Series.
   :type benchmark: Series
   :param join_with: DataFrame to join based on "mention-id". Defaults to None.
   :type join_with: DataFrame, optional

   :returns: DataFrame containing missing cluster links according to the given benchmark.
   :rtype: DataFrame


